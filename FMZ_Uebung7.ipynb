{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# FMZ Übung 7: **Kreuzvalidierung & Bootstrap**\n",
        "\n",
        "---\n",
        "### Ziel der Übung\n",
        "- Überanpassung („Overfitting“) sichtbar machen\n",
        "- Training/Test-Aufteilung anwenden\n",
        "- Bootstrap-Resampling für Schätzung der Fehlervariabilität nutzen\n",
        "- Mehrere Fehlermetriken berechnen\n",
        "- Modelle: Lineare Regression, Polynomregression, Random Forest, SVM, Neural Net\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "1Ca-Urn1b6M8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBrfOUAdTaVG"
      },
      "outputs": [],
      "source": [
        "# Benötigte Pakete\n",
        "\n",
        "install.packages(c(\"ggplot2\",\"dplyr\",\"randomForest\",\"e1071\",\"neuralnet\"))\n",
        "\n",
        "library(ggplot2)\n",
        "library(dplyr)\n",
        "library(randomForest)\n",
        "library(e1071)\n",
        "library(neuralnet)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Datensatz erzeugen (synthetisch, nichtlinear)"
      ],
      "metadata": {
        "id": "yS0fFpXVce5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "set.seed(1)\n",
        "n <- 500\n",
        "x <- runif(n, -5, 5)\n",
        "y <- sin(x) + rnorm(n, sd=.5)\n",
        "data <- data.frame(x,y)\n",
        "plot(data)\n"
      ],
      "metadata": {
        "id": "w24M7J4FU2o9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 1. Aufgabe: **Train/Test-Aufteilung & Overfitting**\n",
        "\n",
        "- Erstellen Sie einen 80/20-Datenaufteilung und passen Sie drei Modelle an:\n",
        "  - lineare Regression\n",
        "  - Polynomregression\n",
        "  - Random Forest\n",
        "\n",
        "- Berechnen Sie die R²-Fehlermetrik\n",
        "\n",
        "- Vergleichen Sie Train vs Test, um Overfitting zu zeigen."
      ],
      "metadata": {
        "id": "NKGaP1i7dOYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Aufteilung der Daten\n",
        "set.seed(1)\n",
        "id <- sample(1:nrow(data), 0.8*nrow(data))\n",
        "train <- data[id,]\n",
        "test  <- data[-id,]  #-id: Tipp, um die verbleibenden Beobachtungen zu erhalten\n",
        "\n",
        "#R²-Fehlermetrik\n",
        "R2   <- function(y, yhat) 1 - sum((y-yhat)^2)/sum((y-mean(y))^2)\n",
        "\n",
        "# 1) Lineare Regression\n",
        "mod_lin <- lm(y ~ x, data=train)\n",
        "\n",
        "# 2) Polynomregression\n",
        "mod_poly <- lm(y ~ poly(x,5), data=train)\n",
        "\n",
        "# 3) Random Forest\n",
        "mod_rf <- randomForest(y ~ x, data=train, ntree=200)\n",
        "\n",
        "# Darstellung der Ergebnisse\n",
        "evaluate <- function(model, train, test)\n",
        "  tibble(\n",
        "    R2_train   = R2(train$y, predict(model, train)),\n",
        "    R2_test    = R2(test$y,  predict(model, test)))\n",
        "\n",
        "results <- bind_rows(\n",
        "  evaluate(mod_lin, train, test)  |> mutate(Model=\"Linear\"),\n",
        "  evaluate(mod_poly, train, test) |> mutate(Model=\"Poly\"),\n",
        "  evaluate(mod_rf, train, test)   |> mutate(Model=\"RF\"))\n",
        "\n",
        "print(results)\n"
      ],
      "metadata": {
        "id": "6QIrtmybU7at"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Graphische Darstellung der Ergebnisse\n",
        "model_list <- list(\"Linear\" = mod_lin, \"Polynom\" = mod_poly, \"Random Forest\" = mod_rf)\n",
        "\n",
        "for(name in names(model_list)){\n",
        "  mod <- model_list[[name]]\n",
        "\n",
        "  # Vorhersagen\n",
        "  yt_tr <- predict(mod, train)\n",
        "  yt_ts <- predict(mod, test)\n",
        "\n",
        "  # Plot\n",
        "  plot(train$x, train$y, pch=19, col=\"grey60\", main=name, xlab=\"x\", ylab=\"y\")\n",
        "  points(train$x, yt_tr, col=\"blue\", pch=16)\n",
        "  points(test$x, test$y, col=\"grey60\", pch=1)\n",
        "  points(test$x, yt_ts, col=\"red\", pch=16)\n",
        "  legend(\"bottomleft\", legend=c(paste(\"Train R² =\", round(R2(train$y, yt_tr),3)), paste(\"Test R² =\", round(R2(test$y, yt_ts),3))), bty=\"n\")\n",
        "}"
      ],
      "metadata": {
        "id": "1v-D1pCxbfSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 2. Aufgabe: **Bootstrap-Cross-Validation**\n",
        "\n",
        "Jetzt wird eine Bootstrap-Schleife (B=100) implementiert.\n",
        "In jeder Iteration:\n",
        "- Zufällige Ziehung von 80% der Daten als Training\n",
        "- Modell schätzen\n",
        "- Training/Test-Fehler speichern\n",
        "\n",
        "So erhalten wir Verteilungen für die Fehlermetriken statt Punktschätzungen."
      ],
      "metadata": {
        "id": "ZQEucEGtg0I3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Speichern der Fehlermetriken jeder Iteration\n",
        "B <- 100\n",
        "store <- data.frame(\n",
        "  R2_test_lin = numeric(B),\n",
        "  R2_test_poly = numeric(B),\n",
        "  R2_test_rf = numeric(B))\n",
        "\n",
        "set.seed(1)\n",
        "for(b in 1:B){\n",
        "\n",
        "  # Train/Test-Aufteilung der Daten\n",
        "  id <- sample(1:nrow(data), 0.8*nrow(data))\n",
        "  train <- data[id,]\n",
        "  test  <- data[-id,]\n",
        "\n",
        "  # Modelle\n",
        "  m_lin  <- lm(y ~ x, data=train)\n",
        "  m_poly <- lm(y ~ poly(x,8), data=train)\n",
        "  m_rf   <- randomForest(y ~ x, data=train, ntree=200)\n",
        "\n",
        "  # Fehler speichern\n",
        "  store$R2_test_lin[b]  <- R2(test$y, predict(m_lin, test))\n",
        "  store$R2_test_poly[b] <- R2(test$y, predict(m_poly, test))\n",
        "  store$R2_test_rf[b]   <- R2(test$y, predict(m_rf, test))\n",
        "}\n",
        "\n",
        "# Graphische Darstellung der R²-Verteilungen (Boxplot + Histogramm)\n",
        "df_long <- data.frame(\n",
        "  Model = rep(names(store), each = nrow(store)),\n",
        "  R2    = as.vector(as.matrix(store)))\n",
        "\n",
        "ggplot(df_long, aes(x = Model, y = R2, fill = Model)) +\n",
        "  geom_boxplot() +\n",
        "  theme_bw()\n",
        "\n",
        "ggplot(df_long, aes(R2, fill = Model)) +\n",
        "  geom_histogram(alpha=.5, bins=30, position=\"identity\") +\n",
        "  theme_bw()\n"
      ],
      "metadata": {
        "id": "_8w-t6hwhMJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 3. Aufgabe: **Vergleich unterschiedlicher Fehlermetriken**\n",
        "\n",
        "- RMSE (root mean square error)\n",
        "- MAE (mean absolute error)\n",
        "- R² (Bestimmtheitsmaß)"
      ],
      "metadata": {
        "id": "LifiazIziWNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Fehlermetriken\n",
        "RMSE <- function(y, yhat) sqrt(mean((y - yhat)^2))\n",
        "MAE  <- function(y, yhat) mean(abs(y - yhat))\n",
        "R2   <- function(y, yhat) 1 - sum((y - yhat)^2) / sum((y - mean(y))^2)\n",
        "\n",
        "# Beispiel für Random Forest\n",
        "set.seed(2)\n",
        "id <- sample(1:nrow(data), 0.8*nrow(data))\n",
        "  train <- data[id,]\n",
        "  test  <- data[-id,]\n",
        "\n",
        "  m <- randomForest(y ~ x, data=train, ntree=500)\n",
        "  #m <- lm(y ~ poly(x,8), data=train)\n",
        "  pred_train <- predict(m, train)\n",
        "  pred_test <- predict(m, test)\n",
        "\n",
        "  print(paste(\"RMSE-Train:\", round(RMSE(train$y, pred_train), 5)))\n",
        "  print(paste(\"MAE-Train:\", round(MAE(train$y, pred_train), 5)))\n",
        "  print(paste(\"R²-Train:\", round(R2(train$y, pred_train), 5)))\n",
        "\n",
        "  print(paste(\"RMSE-Test:\", round(RMSE(test$y, pred_test), 5)))\n",
        "  print(paste(\"MAE-Test:\", round(MAE(test$y, pred_test), 5)))\n",
        "  print(paste(\"R²-Test:\", round(R2(test$y, pred_test), 5)))\n"
      ],
      "metadata": {
        "id": "_S5EybO6j7Q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 4. Aufgabe: **Hyperparameter-Optimierung**\n",
        "\n",
        "- Wählen Sie:\n",
        "   - Polynomialgrad: 3, 5, 8\n",
        "   - Random Forest: ntree = 50, 200, 500\n",
        "\n",
        "- Verwende eine verschachtelte Bootstrap-Schleife, um die optimalen Hyperparameter zu schätzen."
      ],
      "metadata": {
        "id": "7S96Mc7VnI3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Wahl des Polynomgrads und der Baumanzahl\n",
        "set.seed(1)\n",
        "poly_deg  <- c(3,5,8)\n",
        "rf_ntree  <- c(50,200,500)\n",
        "\n",
        "# Speichern der Fehlermetriken\n",
        "best_poly <- numeric(B)\n",
        "best_rf   <- numeric(B)\n",
        "\n",
        "# Bootstrap-Schleife\n",
        "B <- 100\n",
        "for(b in 1:B){\n",
        "  id  <- sample(1:nrow(data), 0.8*nrow(data))\n",
        "  tr   <- data[id,]\n",
        "  ts   <- data[-id,]\n",
        "\n",
        "  # Polynomial\n",
        "  rm <- sapply(poly_deg, function(d){\n",
        "    m <- lm(y ~ poly(x,d), data=tr)\n",
        "    RMSE(ts$y, predict(m, ts)) })\n",
        "  best_poly[b] <- poly_deg[ which.min(rm) ]\n",
        "\n",
        "  # Random Forest\n",
        "  rm <- sapply(rf_ntree, function(nt){\n",
        "    m <- randomForest(y ~ x, data=tr, ntree=nt)\n",
        "    RMSE(ts$y, predict(m, ts)) })\n",
        "  best_rf[b] <- rf_ntree[ which.min(rm) ]\n",
        "}\n",
        "\n",
        "table(best_poly)\n",
        "table(best_rf)\n"
      ],
      "metadata": {
        "id": "RaEP-WCnmBOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Zusatzaufgabe: **Anwendung auf realen Daten I**\n",
        "\n",
        "- Verwendung des Datensatzes `mtcars`:\n",
        "   - y = `mpg`\n",
        "   - x = `hp`, `wt`, `disp`\n",
        "\n",
        "- Setzen Sie Bootstrap-Cross-Validation ein, um die Genauigkeit mehrerer Modelle zu vergleichen."
      ],
      "metadata": {
        "id": "U0tNOxW5oGgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Aufbau des Datensatzes\n",
        "set.seed(1)\n",
        "summary(mtcars)\n",
        "df <- mtcars[,c(\"mpg\",\"hp\",\"wt\",\"disp\")]\n",
        "B <- 100\n",
        "\n",
        "# Speichern der Fehlermetriken\n",
        "store_RMSE <- data.frame(\n",
        "  lin=numeric(B),\n",
        "  rf=numeric(B),\n",
        "  svm=numeric(B))\n",
        "store_R2 <- data.frame(\n",
        "  lin=numeric(B),\n",
        "  rf=numeric(B),\n",
        "  svm=numeric(B))\n",
        "\n",
        "# Bootstrap-Schleife\n",
        "for(b in 1:B){\n",
        "  idx <- sample(1:nrow(df), 0.8*nrow(df), replace=TRUE)\n",
        "  tr  <- df[idx,]\n",
        "  ts  <- df[-unique(idx),]\n",
        "\n",
        "  # Linear\n",
        "  m1 <- lm(mpg ~ ., data=tr)\n",
        "  store_RMSE$lin[b] <- RMSE(ts$mpg, predict(m1, ts))\n",
        "  store_R2$lin[b] <- R2(ts$mpg, predict(m1, ts))\n",
        "\n",
        "  # Random Forest\n",
        "  m2 <- randomForest(mpg ~ ., data=tr, ntree=300)\n",
        "  store_RMSE$rf[b] <- RMSE(ts$mpg, predict(m2, ts))\n",
        "  store_R2$rf[b] <- R2(ts$mpg, predict(m2, ts))\n",
        "\n",
        "  # SVM\n",
        "  m3 <- svm(mpg ~ ., data=tr, cost=1)\n",
        "  store_RMSE$svm[b] <- RMSE(ts$mpg, predict(m3, ts))\n",
        "  store_R2$svm[b] <- R2(ts$mpg, predict(m3, ts))\n",
        "}\n",
        "\n",
        "boxplot(store_RMSE, main=\"Bootstrap RMSE – Vergleich (mtcars)\")\n",
        "boxplot(store_R2, main=\"Bootstrap R² – Vergleich (mtcars)\")\n"
      ],
      "metadata": {
        "id": "nIVrZ7KKn2vL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Zusatzaufgabe: **Anwendung auf realen Daten II**\n",
        "\n",
        "Fußgänger-Datensatz (Vorlesungsfolien)"
      ],
      "metadata": {
        "id": "XWDBc-GHpfha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Lesen der Daten\n",
        "ped <- read.table(\"https://raw.githubusercontent.com/antoinetordeux/Datasets/refs/heads/main/ped_data.txt\", header=TRUE)\n",
        "head(ped)\n",
        "attach(ped)\n",
        "\n",
        "# Graphische Darstellung der Ergebnisse\n",
        "par(mfrow=c(2,2))\n",
        "plot_res=function(algo,titel,cc){\n",
        "\tfor(tt in c(titel, \"Testing\")){\n",
        "\t\tplot(X[cc,1], Y[cc], xlab=\"X: Spacing (m)\", ylab=\"Y: Speed (m/s)\", main=tt)\n",
        "\t\tlines(sort(Spacing[cc]), predict(algo,as.data.frame(X[cc,]))[order(Spacing[cc])], col=\"blue\", lwd=1)\n",
        "\t\tlegend(\"bottomright\", paste('R² =', round(R2(Y[cc],predict(algo,as.data.frame(X[cc,]))), 3)), bty='n')\n",
        "\t\tcc=!cc}}\n",
        "\n",
        "\n",
        "# Datenaufteilung\n",
        "X=cbind(Spacing,Spacing_pred,Speed_pred,Acceleration)\n",
        "Y=Speed\n",
        "n=length(Y)\n",
        "cc=NULL;cc[1:n]=T;cc[sample(1:n,.2*n)]=F\n",
        "\n",
        "# Multiple lineare Regression\n",
        "algo_MLR=lm(Y[cc]~.,data=as.data.frame(X[cc,]))\n",
        "plot_res(algo_MLR,expression(paste(italic(Y)==italic(a[1]*X[1]+...+a[n]*X[n]+b))),cc)\n",
        "\n",
        "# Neural network\n",
        "algo_NN=neuralnet(Y[cc]~.,data=X[cc,],h=c(1,2))\n",
        "plot_res(algo_NN,expression(\"Neural network c(1,2)\"),cc)\n",
        "\n",
        "# Random Forest\n",
        "algo_RF=randomForest(Y[cc]~.,data=X[cc,])\n",
        "plot_res(algo_RF,expression(\"Random forest\"),cc)\n",
        "\n",
        "# Support Vector Machine\n",
        "algo_SVM=svm(Y[cc]~.,data=X[cc,])\n",
        "plot_res(algo_SVM,expression(\"Support vector machine\"),cc)\n",
        "\n",
        "detach()\n"
      ],
      "metadata": {
        "id": "4Pl9xQFcolYw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}